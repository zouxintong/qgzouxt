**梯度下降**：

1. **参数更新规则**：

   在每次迭代中，根据梯度方向更新模型参数。参数更新规则如下：
   $$\theta_j := \theta_j - \alpha \frac{\partial J(\theta)}{\partial \theta_j} 
   其中， \alpha 是学习率，用来控制参数更新的步长.$$

2. **梯度计算**：

   梯度表示了损失函数在当前参数值处的变化率。对于损失函数$J(\theta)$，其梯度是一个向量，其中每个分量$\frac{\partial J(\theta)}{\partial \theta_j}$表示了损失函数对于参数$\theta_j$的变化率。

   在每次迭代中，需要计算损失函数关于参数的偏导数，即梯度。对于不同的损失函数和模型，梯度的计算方式会有所不同。通常采用的方法是使用链式法则来计算。

3. **参数更新**：

   通过将当前参数值沿着负梯度方向移动一定步长来更新参数。这个步长由学习率$ \alpha $控制。学习率越大，参数更新的步长越大，但可能会导致不稳定性和震荡；学习率越小，参数更新的步长越小，但可能需要更多的迭代次数才能收敛到最优解。

   更新规则为：$\theta_j := \theta_j - \alpha \frac{\partial J(\theta)}{\partial \theta_j}$

4. **迭代过程**：

   迭代过程会一直进行直到满足停止条件，例如达到最大迭代次数或损失函数变化很小时停止。每一次迭代，模型都会朝着损失函数减小的方向更新参数，直到找到损失函数的局部最小值或全局最小值。

梯度下降是一种常用的优化算法，适用于各种机器学习模型和任务中，包括线性回归、逻辑回归、神经网络等。
